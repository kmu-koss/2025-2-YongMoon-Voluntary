{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FOxAq5f4_fD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58361f22-13e9-4570-b815-08bd65bde964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "환경 로드 성공!\n"
          ]
        }
      ],
      "source": [
        "from pong_game import PongEnv  # 파일명에서 .py를 제외하고 임포트\n",
        "env = PongEnv(render_mode=None)\n",
        "print(\"환경 로드 성공!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, collections\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"라이브러리 로드 완료!\")"
      ],
      "metadata": {
        "id": "91dPuCzX_iQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 구조 정의\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNet, self).__init__()\n",
        "        # TODO 1-1\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO 1-2\n",
        "        return self.fc3(x)\n",
        "\n",
        "    def sample_action(self, obs, epsilon):\n",
        "        # 모험(Exploration) vs 선택(Exploitation)\n",
        "        # TODO 2\n",
        "        out = self.forward(obs)\n",
        "        if\n",
        "            return\n",
        "        else:\n",
        "            return\n",
        "\n",
        "print(\"신경망 구조 정의 완료!\")"
      ],
      "metadata": {
        "id": "u-0EmOQJLJOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(q, q_target, memory, optimizer):\n",
        "    # 메모리에서 랜덤하게 경험 추출\n",
        "    s, a, r, s_prime, done_mask = zip(*random.sample(memory, batch_size))\n",
        "\n",
        "    # 데이터를 텐서로 변환 (속도 최적화)\n",
        "    s = torch.tensor(np.array(s), dtype=torch.float)\n",
        "    a = torch.tensor(a).unsqueeze(1)\n",
        "    r = torch.tensor(r, dtype=torch.float).unsqueeze(1)\n",
        "    s_prime = torch.tensor(np.array(s_prime), dtype=torch.float)\n",
        "    done_mask = torch.tensor(done_mask, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "    # Q-러닝 오차 계산\n",
        "    q_out = q(s)\n",
        "    q_a = q_out.gather(1, a)\n",
        "    max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
        "\n",
        "    # TODO 3-1\n",
        "    target =\n",
        "\n",
        "    loss = F.mse_loss(q_a, target)\n",
        "\n",
        "    # 신경망 업데이트\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "7k-Y2LjoLLEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = QNet()\n",
        "q_target = QNet()\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
        "memory = collections.deque(maxlen=buffer_limit)\n",
        "score_history = []  # 점수 기록용\n",
        "\n",
        "print(\"AI 탄생! 이제 학습을 시작할 준비가 되었습니다.\")"
      ],
      "metadata": {
        "id": "tc_2MYnjLMBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터\n",
        "learning_rate = 0.001\n",
        "gamma         = 0.99\n",
        "batch_size    = 64\n",
        "buffer_limit  = 10000\n",
        "\n",
        "# 모델 초기화\n",
        "q = QNet()\n",
        "q_target = QNet()\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "\n",
        "# 최적화 도구 재설정\n",
        "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
        "\n",
        "# 기억 장치 초기화\n",
        "memory = collections.deque(maxlen=50000)\n",
        "\n",
        "# 그래프용 기록 초기화\n",
        "score_history = []\n",
        "max_avg_score = 0"
      ],
      "metadata": {
        "id": "2f5mJepRIWDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_res(scores, n_epi, num_episodes, max_avg_score, epsilon, memory_len):\n",
        "    display.clear_output(wait=True)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(scores, label='Avg Score (per 10 episodes)', color='b', linewidth=2)\n",
        "    plt.title(f'DQN Training Progress (Episode: {n_epi})')\n",
        "    plt.xlabel('Session Count (x10 episodes)')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
        "    print(f\"[학습 현황] 에피소드: {n_epi} / {num_episodes}\")\n",
        "    print(f\"현재 평균 점수: {scores[-1]:7.1f} 점\")\n",
        "    print(f\"역대 최고 점수: {max_avg_score:7.1f} 점\")\n",
        "    print(f\"모험 확률(ε): {epsilon:7.2%} (학습할수록 낮아집니다)\")\n",
        "    print(f\"기억 저장소: {memory_len:7d} / 50000 개\")\n",
        "    print(f\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\")\n",
        "\n",
        "num_episodes = 200\n",
        "env = PongEnv(render_mode=None)\n",
        "current_total_score = 0\n",
        "max_avg_score = 0\n",
        "\n",
        "print(\"학습 준비 중...\")\n",
        "\n",
        "for n_epi in range(1, num_episodes + 1):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # 학습이 진행될수록(score_history가 쌓일수록) 모험심(epsilon) 줄이기\n",
        "        total_steps = len(score_history) * 10 + n_epi\n",
        "\n",
        "        # TODO 4\n",
        "        epsilon =\n",
        "\n",
        "        a = q.sample_action(torch.from_numpy(s).float(), epsilon)\n",
        "        s_prime, r, done, info = env.step(a)\n",
        "\n",
        "        # 경험 저장\n",
        "        memory.append((s, a, r, s_prime, 0.0 if done else 1.0))\n",
        "        s = s_prime\n",
        "        current_total_score += info['score']\n",
        "\n",
        "        # 경험이 쌓이면 학습 시작\n",
        "        if len(memory) > 1000:\n",
        "            train(q, q_target, memory, optimizer)\n",
        "\n",
        "    # 10번의 에피소드마다 그래프와 수치를 갱신\n",
        "    if n_epi % 10 == 0:\n",
        "        avg_score = current_total_score / 10\n",
        "        score_history.append(avg_score)\n",
        "\n",
        "        # 최고 점수 갱신 체크\n",
        "        if avg_score > max_avg_score:\n",
        "            max_avg_score = avg_score\n",
        "\n",
        "        q_target.load_state_dict(q.state_dict()) # 타겟 네트워크 동기화\n",
        "\n",
        "        plot_res(score_history, n_epi, num_episodes, max_avg_score, epsilon, len(memory))\n",
        "\n",
        "        current_total_score = 0\n",
        "\n",
        "# 최종 저장\n",
        "print(\"\\n학습 완료!\")\n",
        "print(f\"최종 최고 기록: {max_avg_score}점\")"
      ],
      "metadata": {
        "id": "CGGxClrmLOIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(q.state_dict(), \"pong_model.pth\")\n",
        "print(\"모델이 'pong_model.pth'로 성공적으로 저장되었습니다. 왼쪽 파일 탭에서 다운로드하세요!\")"
      ],
      "metadata": {
        "id": "aypxFL0jaYRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}